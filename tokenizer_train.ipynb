{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9ff300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from tokenizers import decoders, models, pre_tokenizers, trainers, Tokenizer\n",
    "import os\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4dfb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def sample_10_percent(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as fout:\n",
    "        \n",
    "        for i, line in enumerate(fin):\n",
    "            if i % 10 == 0:  # 每10行取1行\n",
    "                try:\n",
    "                    json.loads(line)  # 验证JSON有效性\n",
    "                    fout.write(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # 跳过损坏行\n",
    "\n",
    "# 执行抽样\n",
    "sample_10_percent('./dataset/tokenizer_training_data.jsonl', './dataset/sampled_10p.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e95c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./dataset/sampled_10p.jsonl\"\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff92a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = read_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a632561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "\n",
    "#定义特殊的Token\n",
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18076ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置训练器并加载特殊token\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ec08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练tokenizer\n",
    "tokenizer.train_from_iterator(texts,trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548cc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "#检查特殊token的索引\n",
    "assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "assert tokenizer.token_to_id(\"</s>\") == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db61867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer/vocab.json', './tokenizer/merges.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存tokenizer\n",
    "tokenizer_dir = \"./tokenizer/\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(\"./tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc75294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# 手动创建配置文件\n",
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<|endoftext|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<|im_start|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"<|im_end|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<|endoftext|>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}{% else %}{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2.7.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
